{"cells":[{"cell_type":"markdown","source":["#### Bronze Layer"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4f575aa0-48eb-445b-b6a8-89ccb45ebaf6"},{"cell_type":"code","source":["from pyspark.sql.functions import col, explode, arrays_zip, to_date, map_keys, current_timestamp\n","from pyspark.sql.types import TimestampType\n","import json\n","from pyspark.sql.functions import map_from_entries, struct, lit\n","from pyspark.sql.functions import to_timestamp, from_unixtime\n","from pyspark.sql import DataFrame\n","from functools import reduce\n","\n","# Pfade\n","news_path = \"Files/news_data.json\"\n","data_path = \"abfss://FinanceProject@onelake.dfs.fabric.microsoft.com/finance_lakehouse.Lakehouse/Files\"\n","\n","# News Data zu DF\n","df = spark.read.option(\"multiline\", \"true\").json(news_path)\n","\n","df_news = df.selectExpr(\"explode(articles) as article\").select(\n","    col(\"article.source.name\").cast(\"string\").alias(\"name\"),\n","    col(\"article.url\").cast(\"string\").alias(\"url\"),\n","    col(\"article.urlToImage\").cast(\"string\").alias(\"urlToImage\"),\n","    col(\"article.content\").cast(\"string\").alias(\"content\"),\n","    col(\"article.title\").cast(\"string\").alias(\"title\"),\n","    col(\"article.publishedAt\").cast(TimestampType()).alias(\"PublishDate\"),\n",")\n","\n","df_news = df_news.withColumn(\"ingestion_time\", current_timestamp())\n","df_news.write.mode(\"overwrite\").saveAsTable(\"news_data_bronze\")\n","\n","# Liste der Dateien im Verzeichnis\n","files = mssparkutils.fs.ls(data_path)\n","\n","# History Data zu DF\n","relevant_files = [file.path for file in files if \"history_data\" in file.name.lower()]\n","\n","for file_path in relevant_files:\n","    try:\n","        # Lese die JSON-Datei\n","        df_raw = spark.read.option(\"multiline\", \"true\").json(file_path)\n","\n","        # Extrahiere Metadaten\n","        df_meta = df_raw.select(\n","            col(\"Meta Data\").getItem(\"2. Symbol\").cast(\"string\").alias(\"symbol\"),\n","            col(\"Meta Data\").getItem(\"4. Interval\").cast(\"string\").alias(\"interval\")\n","        )\n","\n","        # Extrahiere den ticker als String\n","        file_name = file_path.split(\"/\")[-1]\n","        file_name_without_extension = file_name.replace(\".json\", \"\")\n","        ticker = file_name_without_extension.split(\"_\")[0]\n","        \n","        # Extrahiere die Zeitreihen-Daten\n","        df_time = df_raw.select(\"`Time Series (5min)`\")\n","        df_time = df_time.withColumnRenamed(\"Time Series (5min)\", \"time_struct\")\n","\n","        # Konvertiere die Map in ein RDD und dann in einen DataFrame\n","        time_map = df_time.rdd.flatMap(lambda row: row[0].asDict().items())\n","        df_temp = spark.createDataFrame(time_map, [\"timestamp\", \"data\"])\n","\n","        # Extrahiere die Werte aus der Struct-Spalte\n","        df_data = df_temp.select(\n","            col(\"timestamp\"),\n","            col(\"data\").getItem(\"2. high\").cast(\"double\").alias(\"high\"),\n","            col(\"data\").getItem(\"3. low\").cast(\"double\").alias(\"low\"),\n","            col(\"data\").getItem(\"5. volume\").cast(\"double\").alias(\"volume\"),\n","            col(\"data\").getItem(\"1. open\").cast(\"double\").alias(\"open\"),\n","            col(\"data\").getItem(\"4. close\").cast(\"double\").alias(\"close\")\n","        )\n","\n","        # Führe einen Cross-Join durch, um die Metadaten hinzuzufügen\n","        df_final = df_data.crossJoin(df_meta)\n","\n","        # Erstelle den Tabellennamen mit dem korrekten ticker\n","        file_name = f\"{ticker}_historic_data_bronze\"\n","\n","        df_final = df_final.withColumn(\"ingestion_time\", current_timestamp())\n","\n","        # Schreibe die Daten in die Tabelle\n","        df_final.write.mode(\"overwrite\").saveAsTable(file_name)\n","\n","        print(f\"Daten in Tabelle {file_name} geschrieben.\")\n","\n","    except Exception as e:\n","        print(f\"Fehler bei der Verarbeitung von {file_path}: {str(e)}\")\n","        continue\n","\n","# Real-time Data zu DF\n","relevant_files = [file.path for file in files if \"realtime_data\" in file.name.lower()]\n","\n","for file_path in relevant_files:\n","    try:\n","        # Lese die JSON-Datei\n","        df_raw = spark.read.option(\"multiline\", \"true\").json(file_path)\n","\n","        # Extrahiere die Real-time-Daten\n","        df_data = df_raw.select(\n","            col(\"c\").cast(\"double\").alias(\"current_price\"), \n","            col(\"d\").cast(\"double\").alias(\"change\"),\n","            col(\"dp\").cast(\"double\").alias(\"percentage_change\"), \n","            col(\"h\").cast(\"double\").alias(\"high\"),\n","            col(\"l\").cast(\"double\").alias(\"low\"),\n","            col(\"o\").cast(\"double\").alias(\"open\"),\n","            col(\"pc\").cast(\"double\").alias(\"previous_close\"),  \n","            to_timestamp(from_unixtime(col(\"t\").cast(\"bigint\"))).alias(\"timestamp\")\n","        )\n","\n","        # Extrahiere den ticker aus dem Dateipfad\n","        file_name = file_path.split(\"/\")[-1]\n","        file_name_without_extension = file_name.replace(\".json\", \"\")\n","        ticker = file_name_without_extension.split(\"_\")[0]\n","\n","        # Erstelle den Tabellennamen\n","        file_name = f\"{ticker}_realtime_data_bronze\"\n","\n","        df_data = df_data.withColumn(\"ingestion_time\", current_timestamp())\n","\n","        # Schreibe die Daten in die Tabelle\n","        df_data.write.mode(\"overwrite\").saveAsTable(file_name)\n","\n","        print(f\"Daten in Tabelle {file_name} geschrieben.\")\n","\n","    except Exception as e:\n","        print(f\"Fehler bei der Verarbeitung von {file_path}: {str(e)}\")\n","        continue"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"499df2cb-265e-46d9-b814-ac16f123eefd","normalized_state":"finished","queued_time":"2025-04-09T15:55:56.282749Z","session_start_time":null,"execution_start_time":"2025-04-09T15:55:56.2841175Z","execution_finish_time":"2025-04-09T15:57:02.0797061Z","parent_msg_id":"6433c875-f022-496e-ab3c-000c3196da45"},"text/plain":"StatementMeta(, 499df2cb-265e-46d9-b814-ac16f123eefd, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Daten in Tabelle QQQ_historic_data_bronze geschrieben.\nDaten in Tabelle SPY_historic_data_bronze geschrieben.\nDaten in Tabelle QQQ_realtime_data_bronze geschrieben.\nDaten in Tabelle SPY_realtime_data_bronze geschrieben.\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"ed193861-c10e-47df-8bf3-253102a423fa"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"1631f146-6ab7-4acf-b8ea-619eba023189"}],"default_lakehouse":"1631f146-6ab7-4acf-b8ea-619eba023189","default_lakehouse_name":"finance_lakehouse","default_lakehouse_workspace_id":"e351f4fd-db59-4c5e-9ef7-9f3e6ee9a6b4"}}},"nbformat":4,"nbformat_minor":5}