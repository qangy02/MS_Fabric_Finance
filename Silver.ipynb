{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import col, to_timestamp, lit, lag, last, mean, when, floor, concat, regexp_extract, regexp_replace\n","from pyspark.sql.window import Window\n","from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n","\n","# Pfad zu den Dateien\n","path = \"Files\"\n","symbols_array = symbols.split(\",\")\n","\n","# Datentypen\n","data_type = [\"historic\", \"realtime\"]\n","\n","# Window-Spezifikation\n","window_spec = Window.partitionBy(\"symbol\").orderBy(\"timestamp\")\n","\n","# Liste für die kombinierten DataFrames\n","all_dfs = []\n","\n","# Schleife über jedes Symbol\n","for symbol in symbols_array:\n","  \n","    # Initialisiere DataFrames für historic und realtime\n","    df_transformed_h = None\n","    df_transformed_r = None\n","    \n","    # Lade die relevanten Dateien für das Symbol\n","    files = mssparkutils.fs.ls(path)\n","    relevant_files = [file.path for file in files if symbol in file.name.lower()]\n","    \n","    # Schleife über die Datentypen (historic und realtime)\n","    for type in data_type:\n","        # Finde die passende Datei für den Datentyp\n","        for file_path in relevant_files:\n","            index = file_path.split(\"/\")[-1].split(\"_\")[0].lower()\n","            table_name = f\"finance_lakehouse.dbo.{index}_{type}_data_bronze\"\n","            \n","            try:\n","                df = spark.read.table(table_name)\n","                \n","                if type == \"realtime\":\n","                    # Transformation für Realtime-Daten\n","                    df_transformed_r = df.select(\n","                        to_timestamp(col(\"timestamp\")).alias(\"timestamp\"),\n","                        col(\"high\").cast(DoubleType()).alias(\"high\"),\n","                        col(\"low\").cast(DoubleType()).alias(\"low\"),\n","                        col(\"current_price\").cast(DoubleType()).alias(\"close\"),\n","                        col(\"open\").cast(DoubleType()).alias(\"open\"),\n","                        lit(None).cast(DoubleType()).alias(\"volume\"),\n","                        lit(symbol.upper()).cast(StringType()).alias(\"symbol\"),\n","                        lit(None).cast(StringType()).alias(\"interval\"),\n","                        to_timestamp(col(\"ingestion_time\")).alias(\"ingestion_time\"),\n","                        col(\"current_price\").cast(DoubleType()).alias(\"current_price\"),\n","                        col(\"change\").cast(DoubleType()).alias(\"change\"),\n","                        col(\"percentage_change\").cast(DoubleType()).alias(\"percentage_change\"),\n","                        col(\"previous_close\").cast(DoubleType()).alias(\"previous_close\")\n","                    )\n","                else:\n","                    # Transformation für Historic-Daten\n","                    df_transformed_h = df.select(\n","                        to_timestamp(col(\"timestamp\")).alias(\"timestamp\"),\n","                        col(\"high\").cast(DoubleType()).alias(\"high\"),\n","                        col(\"low\").cast(DoubleType()).alias(\"low\"),\n","                        col(\"close\").cast(DoubleType()).alias(\"close\"),\n","                        col(\"open\").cast(DoubleType()).alias(\"open\"),\n","                        col(\"volume\").cast(DoubleType()).alias(\"volume\"),\n","                        col(\"symbol\").cast(StringType()).alias(\"symbol\"),\n","                        col(\"interval\").cast(StringType()).alias(\"interval\"),\n","                        to_timestamp(col(\"ingestion_time\")).alias(\"ingestion_time\"),\n","                        col(\"close\").cast(DoubleType()).alias(\"current_price\"),\n","                        lit(None).cast(DoubleType()).alias(\"change\"),\n","                        lit(None).cast(DoubleType()).alias(\"percentage_change\"),\n","                        lit(None).cast(DoubleType()).alias(\"previous_close\")\n","                    )\n","            except Exception as e:\n","                print(f\"Fehler beim Laden der Tabelle {table_name}: {str(e)}\")\n","                continue\n","    \n","    # Kombiniere die DataFrames, wenn beide existieren\n","    if df_transformed_h is not None and df_transformed_r is not None:\n","        unified_df = df_transformed_r.union(df_transformed_h)\n","    elif df_transformed_h is not None:\n","        unified_df = df_transformed_h\n","    elif df_transformed_r is not None:\n","        unified_df = df_transformed_r\n","    else:\n","        continue\n","    \n","    # Transformationen auf das kombinierte DataFrame anwenden\n","    unified_df = unified_df.dropDuplicates([\"timestamp\", \"symbol\"])\n","    unified_df = unified_df.orderBy(\"timestamp\")\n","\n","    # previous_close berechnen (lag von close)\n","    unified_df = unified_df.withColumn(\n","        \"previous_close\",\n","        when(col(\"previous_close\").isNull(), lag(col(\"close\"), 1).over(window_spec))\n","        .otherwise(col(\"previous_close\"))\n","    )\n","    \n","    # change/percentage_change berechnen\n","    unified_df = unified_df.withColumn(\n","        \"change\",\n","        when(col(\"change\").isNull(), col(\"close\") - col(\"previous_close\"))\n","        .otherwise(col(\"change\"))\n","    ).withColumn(\n","        \"percentage_change\",\n","        when(col(\"percentage_change\").isNull(), (col(\"change\") / col(\"previous_close\")) * 100)\n","        .otherwise(col(\"percentage_change\"))\n","    )\n","    \n","    # volume berechnen\n","    unified_df = unified_df.withColumn(\n","        \"volume\",\n","        when(col(\"volume\").isNull(), last(col(\"volume\"), ignorenulls=True).over(window_spec))\n","        .otherwise(col(\"volume\"))\n","    )\n","    \n","    # interval hinzufügen für Realtime-Daten\n","    unified_df = unified_df.withColumn(\n","        \"interval\",\n","        when(col(\"interval\").isNull(), lit(\"5min\")).otherwise(col(\"interval\"))\n","    )\n","    \n","    # change nach 5 Kommastellen abschneiden\n","    unified_df = unified_df.withColumn(\n","        \"change\",\n","        when(col(\"change\").isNotNull(), floor(col(\"change\") * 100000) / 100000)\n","        .otherwise(col(\"change\"))\n","    )\n","    \n","    # percentage_change nach 5 Kommastellen abschneiden\n","    unified_df = unified_df.withColumn(\n","        \"percentage_change\",\n","        when(col(\"percentage_change\").isNotNull(), floor(col(\"percentage_change\") * 100000) / 100000)\n","        .otherwise(col(\"percentage_change\"))\n","    )\n","    \n","    # Speichere das kombinierte DataFrame\n","    file_name = f\"{symbol}_data_silver\"\n","    unified_df.write.mode(\"overwrite\").saveAsTable(file_name)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"0735ed4b-1e7e-4e27-8e82-2818fbeaa3a1","normalized_state":"finished","queued_time":"2025-04-11T10:53:24.5909911Z","session_start_time":"2025-04-11T10:53:24.5922577Z","execution_start_time":"2025-04-11T10:57:45.719858Z","execution_finish_time":"2025-04-11T10:58:30.7827059Z","parent_msg_id":"dc7cb3fb-4c25-4f48-b7b7-6a098f211be3"},"text/plain":"StatementMeta(, 0735ed4b-1e7e-4e27-8e82-2818fbeaa3a1, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"advisor":{"adviceMetadata":"{\"artifactId\":\"0322175d-f06e-45cb-b388-c59a3cf4dcd6\",\"activityId\":\"0735ed4b-1e7e-4e27-8e82-2818fbeaa3a1\",\"applicationId\":\"application_1744368985014_0001\",\"jobGroupId\":\"3\",\"advices\":{\"info\":2}}"}},"id":"f548a3f3-25b7-4b2d-9c87-5f29ea769c2a"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, to_timestamp, regexp_replace, when, length, lit\n","from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n","\n","df = spark.read.table(\"finance_lakehouse.dbo.news_data_bronze\")\n","\n","# Transformationen durchführen\n","df_news = df.select(\n","    col(\"name\").cast(StringType()).alias(\"source\"),\n","    col(\"url\").cast(StringType()).alias(\"url\"),\n","    col(\"urlToImage\").cast(StringType()).alias(\"image\"),\n","    col(\"content\").cast(StringType()).alias(\"content\"),\n","    col(\"title\").cast(StringType()).alias(\"title\"),\n","    to_timestamp(col(\"PublishDate\")).alias(\"publish_date\"),\n","    to_timestamp(col(\"ingestion_time\")).alias(\"ingestion_time\")\n",")\n","\n","# Leere Strings durch NULL ersetzen\n","df_news = df_news.withColumn(\"content\", when(col(\"content\") == \"\", lit(None)).otherwise(col(\"content\"))) \\\n","                 .withColumn(\"title\", when(col(\"title\") == \"\", lit(None)).otherwise(col(\"title\")))\n","\n","# Content-Länge hinzufügen\n","df_news = df_news.withColumn(\"content_length\", length(col(\"content\")).cast(StringType()))\n","\n","# Duplikate entfernen\n","df_news = df_news.dropDuplicates([\"url\"])\n","\n","# Nach publish_date und ingestion_time sortieren\n","df_news = df_news.orderBy(\"publish_date\", \"ingestion_time\")\n","\n","# In die Silver Layer speichern\n","df_news.write.mode(\"overwrite\").saveAsTable(\"news_data_silver\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"0735ed4b-1e7e-4e27-8e82-2818fbeaa3a1","normalized_state":"finished","queued_time":"2025-04-11T10:53:29.7383735Z","session_start_time":null,"execution_start_time":"2025-04-11T10:58:30.7849844Z","execution_finish_time":"2025-04-11T10:58:38.9513873Z","parent_msg_id":"e8668bb6-20bf-4f7c-a8fd-0affa3f14940"},"text/plain":"StatementMeta(, 0735ed4b-1e7e-4e27-8e82-2818fbeaa3a1, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d22ba8a9-71d2-47c0-a4d3-b16132c84640"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"1631f146-6ab7-4acf-b8ea-619eba023189"}],"default_lakehouse":"1631f146-6ab7-4acf-b8ea-619eba023189","default_lakehouse_name":"finance_lakehouse","default_lakehouse_workspace_id":"e351f4fd-db59-4c5e-9ef7-9f3e6ee9a6b4"}}},"nbformat":4,"nbformat_minor":5}