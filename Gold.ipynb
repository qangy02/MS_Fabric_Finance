{"cells":[{"cell_type":"markdown","source":["#### Sentiment berechnen"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6e17d60c-4460-477d-9141-d59daf74cf21"},{"cell_type":"code","source":["from pyspark.sql.functions import pandas_udf, col, concat_ws\n","from pyspark.sql.types import StructType, StructField, StringType, FloatType\n","from transformers import BertTokenizer, BertForSequenceClassification\n","import torch\n","import pandas as pd\n","\n","# FinBERT laden\n","tokenizer = BertTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n","model = BertForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\")\n","\n","# Funktion für FinBERT-Sentiment\n","def finbert_sentiment(text_series: pd.Series) -> pd.DataFrame:\n","    sentiments = []\n","    confidences = []\n","    # Konvertiere alle Einträge zu Strings und ersetze None durch \"\"\n","    text_series = text_series.fillna(\"\").astype(str)\n","    \n","    for text in text_series:\n","        if not text.strip():  # Falls leerer String\n","            sentiments.append(\"neutral\")\n","            confidences.append(0.0)\n","        else:\n","            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n","            with torch.no_grad():\n","                outputs = model(**inputs)\n","            scores = torch.softmax(outputs.logits, dim=1).numpy()[0]\n","            labels = [\"negative\", \"neutral\", \"positive\"]\n","            sentiment = labels[scores.argmax()]\n","            confidence = scores.max()\n","            sentiments.append(sentiment)    \n","            confidences.append(float(confidence))\n","    \n","    return pd.DataFrame({\"sentiment\": sentiments, \"confidence\": confidences})\n","\n","# Schema für die Rückgabe der UDF\n","schema = StructType([\n","    StructField(\"sentiment\", StringType(), True),\n","    StructField(\"confidence\", FloatType(), True)\n","])\n","\n","# Pandas-UDF definieren\n","@pandas_udf(schema)\n","def compute_sentiment(text_series: pd.Series) -> pd.DataFrame:\n","    return finbert_sentiment(text_series)\n","\n","# JSON-Datei einlesen\n","df = spark.read.option(\"multiline\", \"true\").json(\"Files/news_data.json\")\n","\n","# Entferne die Verschachtelung von \"articles\"\n","df_articles = df.selectExpr(\"explode(articles) as article\").select(\n","    \"article.title\",\n","    \"article.description\"\n",")\n","\n","# Kombiniere title und description, ersetze NULL durch leeren String\n","df_articles = df_articles.withColumn(\n","    \"text\",\n","     concat_ws(\" \", col(\"title\").cast(\"string\"), col(\"description\").cast(\"string\"))\n",")\n","\n","# FinBERT-Sentiment berechnen\n","df_with_sentiment = df_articles.withColumn(\n","    \"sentiment_result\",\n","    compute_sentiment(\"text\")\n",").select(\n","    \"title\",\n","    \"description\",\n","    \"sentiment_result.sentiment\",\n","    \"sentiment_result.confidence\"\n",")\n","\n","# Ergebnisse speichern\n","df_with_sentiment.write.mode(\"overwrite\").saveAsTable(\"news_with_sentiment_gold\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":false}},"id":"19f62a1f-1abe-4955-88b5-2aa86d0ce245"},{"cell_type":"markdown","source":["#### Prognosen der Kurse berechnen\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"731d3050-8134-474a-a5d7-9ebb5eafd214"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, lag, avg, when, lead, last\n","from pyspark.sql.window import Window\n","from pyspark.sql.types import StructType\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.regression import GBTRegressor\n","from pyspark.ml.evaluation import RegressionEvaluator\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime, timedelta\n","\n","# SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Symbole\n","symbols_array = symbols.split(\",\")\n","\n","# Sentiment-Daten einlesen\n","df_sentiment = spark.read.table(\"finance_lakehouse.dbo.news_with_sentiment_gold\")\n","\n","# Schritt 1: Allgemeine Marktstimmung berechnen\n","# Numerischen Sentiment-Score berechnen\n","df_sentiment = df_sentiment.withColumn(\n","    \"sentiment_score\",\n","    when(col(\"sentiment\") == \"positive\", 1).when(col(\"sentiment\") == \"negative\", -1).otherwise(0)\n",")\n","\n","# Gewichteten Sentiment-Score berechnen (sentiment_score * confidence)\n","df_sentiment = df_sentiment.withColumn(\n","    \"weighted_sentiment_score\",\n","    col(\"sentiment_score\") * col(\"confidence\")\n",")\n","\n","# Durchschnittlichen gewichteten Sentiment-Score berechnen\n","overall_market_sentiment = df_sentiment.select(avg(\"weighted_sentiment_score\").alias(\"overall_market_sentiment\")).collect()[0][\"overall_market_sentiment\"]\n","\n","# Marktstimmung interpretieren\n","if overall_market_sentiment > 0:\n","    market_mood = \"positive\"\n","elif overall_market_sentiment < 0:\n","    market_mood = \"negative\"\n","else:\n","    market_mood = \"neutral\"\n","\n","print(f\"Allgemeine Marktstimmung: {market_mood} (Score: {overall_market_sentiment})\")\n","\n","# Liste für die Ergebnisse der ersten Prognose\n","first_predictions = []\n","\n","# Schritt 2: Erste Prognose (nur Finanzdaten)\n","for symbol in symbols_array:\n","    print(f\"Erste Prognose für Symbol: {symbol}\")\n","    \n","    # Finanzdaten für das aktuelle Symbol einlesen\n","    df_finance = spark.read.table(f\"finance_lakehouse.dbo.{symbol}_data_silver\")\n","    \n","    # Runde den Zeitstempel auf das nächste 1-Stunden-Intervall\n","    df_finance = df_finance.withColumn(\n","        \"timestamp_1h\",\n","        (col(\"timestamp\").cast(\"long\") / 3600).cast(\"long\") * 3600\n","    ).withColumn(\n","        \"timestamp_1h\",\n","        col(\"timestamp_1h\").cast(\"timestamp\")\n","    )\n","    \n","    # Aggregiere Finanzdaten auf 1-Stunden-Intervalle\n","    df_finance_agg = df_finance.groupBy(\"symbol\", \"timestamp_1h\").agg(\n","        last(\"close\").alias(\"close\"),\n","        avg(\"volume\").alias(\"volume\"),\n","        avg(\"percentage_change\").alias(\"percentage_change\")\n","    ).orderBy(\"timestamp_1h\")\n","    \n","    # Feature-Engineering\n","    window_spec = Window.partitionBy(\"symbol\").orderBy(\"timestamp_1h\")\n","    \n","    # Lagged Features (z. B. close der letzten 5 Stunden)\n","    for i in range(1, 6):\n","        df_finance_agg = df_finance_agg.withColumn(\n","            f\"close_lag_{i}\",\n","            lag(col(\"close\"), i).over(window_spec)\n","        ).withColumn(\n","            f\"volume_lag_{i}\",\n","            lag(col(\"volume\"), i).over(window_spec)\n","        )\n","    \n","    # Gleitender Durchschnitt (5-Stunden-Durchschnitt von close)\n","    df_finance_agg = df_finance_agg.withColumn(\n","        \"close_ma_5\",\n","        avg(col(\"close\")).over(window_spec.rowsBetween(-4, 0))\n","    )\n","    \n","    # Zielvariable: close-Preis der nächsten Stunde\n","    df_finance_agg = df_finance_agg.withColumn(\n","        \"future_close_1h\",\n","        lead(col(\"close\"), 1).over(window_spec)\n","    )\n","    \n","    # Fehlende Werte behandeln (nur für Feature-Spalten)\n","    feature_columns = [\n","        \"close\", \"volume\", \"percentage_change\",\n","        \"close_lag_1\", \"close_lag_2\", \"close_lag_3\", \"close_lag_4\", \"close_lag_5\",\n","        \"volume_lag_1\", \"volume_lag_2\", \"volume_lag_3\", \"volume_lag_4\", \"volume_lag_5\",\n","        \"close_ma_5\"\n","    ]\n","    df_finance_agg = df_finance_agg.fillna(0, subset=feature_columns)\n","    \n","    # VectorAssembler für die Features\n","    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n","    df_features = assembler.transform(df_finance_agg)\n","    \n","    # Für das Training: Zeilen entfernen, bei denen future_close_1h null ist\n","    df_features_for_training = df_features.filter(col(\"future_close_1h\").isNotNull())\n","    \n","    # Train-Test-Split (z. B. 80% Training, 20% Test)\n","    train_data, test_data = df_features_for_training.randomSplit([0.8, 0.2], seed=42)\n","    \n","    # XGBoost-Modell (in Spark als GBTRegressor implementiert)\n","    gbt = GBTRegressor(\n","        featuresCol=\"features\",\n","        labelCol=\"future_close_1h\",\n","        maxIter=50,\n","        maxDepth=5,\n","        seed=42\n","    )\n","    \n","    # Modell trainieren\n","    model = gbt.fit(train_data)\n","    \n","    # Prognosen auf Testdaten erstellen\n","    predictions = model.transform(test_data)\n","    \n","    # Evaluierung\n","    evaluator = RegressionEvaluator(labelCol=\"future_close_1h\", predictionCol=\"prediction\", metricName=\"rmse\")\n","    rmse = evaluator.evaluate(predictions)\n","    print(f\"RMSE für {symbol} (Erste Prognose): {rmse}\")\n","    \n","    # Für die Prognose: Zeilen auswählen, bei denen future_close_1h null ist\n","    latest_data = df_features.filter(col(\"future_close_1h\").isNull()).orderBy(\"timestamp_1h\", ascending=False).limit(1)\n","    \n","    # Überprüfen, ob latest_data leer ist\n","    if latest_data.count() == 0:\n","        raise ValueError(f\"Keine Daten mit future_close_1h = null für Symbol {symbol} gefunden. Überprüfe die Daten oder die Logik.\")\n","    \n","    latest_timestamp = latest_data.select(\"timestamp_1h\").collect()[0][\"timestamp_1h\"]\n","    \n","    # Iterativ prognostizieren für die nächsten 168 Stunden\n","    current_data = latest_data\n","    predictions_list = []\n","    \n","    for i in range(168):\n","        # Prognose für die nächste Stunde\n","        prediction = model.transform(current_data)\n","        \n","        # Prognostizierten close-Preis extrahieren\n","        predicted_close = prediction.select(\"prediction\").collect()[0][\"prediction\"]\n","        \n","        # Werte für die Feature-Spalten einmalig sammeln\n","        current_row = current_data.collect()[0]\n","        close_lag_1 = current_row[\"close_lag_1\"]\n","        close_lag_2 = current_row[\"close_lag_2\"]\n","        close_lag_3 = current_row[\"close_lag_3\"]\n","        close_lag_4 = current_row[\"close_lag_4\"]\n","        close_lag_5 = current_row[\"close_lag_5\"]\n","        volume_lag_1 = current_row[\"volume_lag_1\"]\n","        volume_lag_2 = current_row[\"volume_lag_2\"]\n","        volume_lag_3 = current_row[\"volume_lag_3\"]\n","        volume_lag_4 = current_row[\"volume_lag_4\"]\n","        volume_lag_5 = current_row[\"volume_lag_5\"]\n","        close_ma_5 = (close_lag_1 + close_lag_2 + close_lag_3 + close_lag_4 + predicted_close) / 5\n","        \n","        # Neue Zeile für die Prognose erstellen (ohne die features-Spalte im Tupel)\n","        new_timestamp = latest_timestamp + timedelta(hours=i + 1)\n","        new_row_data = [\n","            symbol,\n","            new_timestamp,\n","            predicted_close,\n","            0.0,\n","            0.0,\n","            close_lag_1,\n","            close_lag_2,\n","            close_lag_3,\n","            close_lag_4,\n","            close_lag_5,\n","            volume_lag_1,\n","            volume_lag_2,\n","            volume_lag_3,\n","            volume_lag_4,\n","            volume_lag_5,\n","            close_ma_5,\n","            None\n","        ]\n","        \n","        # Schema ohne die features-Spalte erstellen\n","        new_row_schema = StructType([field for field in df_features.schema if field.name != \"features\"])\n","        new_row = spark.createDataFrame([new_row_data], schema=new_row_schema)\n","        \n","        # VectorAssembler anwenden, um die features-Spalte hinzuzufügen\n","        assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n","        new_row = assembler.transform(new_row)\n","        \n","        # Prognose speichern\n","        predictions_list.append(spark.createDataFrame([(\n","            symbol,\n","            new_timestamp,\n","            predicted_close\n","        )], schema=[\"symbol\", \"timestamp_1h\", \"predicted_future_close\"]))\n","        \n","        # Features für die nächste Iteration aktualisieren\n","        current_data = new_row\n","    \n","    # Prognosen für das aktuelle Symbol zusammenführen\n","    symbol_predictions = predictions_list[0]\n","    for df in predictions_list[1:]:\n","        symbol_predictions = symbol_predictions.union(df)\n","    \n","    first_predictions.append(symbol_predictions)\n","\n","# Alle Prognosen der ersten Runde zusammenführen\n","first_predictions_df = first_predictions[0]\n","for df in first_predictions[1:]:\n","    first_predictions_df = first_predictions_df.union(df)\n","\n","# Inhalt von first_predictions_df überprüfen\n","print(\"Inhalt von first_predictions_df:\")\n","first_predictions_df.show(5)\n","\n","# Schritt 3: Finale Prognose (mit allgemeiner Marktstimmung)\n","print(\"Finale Prognose mit allgemeiner Marktstimmung...\")\n","\n","# Liste für die finalen Prognosen\n","final_predictions = []\n","\n","for symbol in symbols_array:\n","    print(f\"Finale Prognose für Symbol: {symbol}\")\n","    \n","    # Finanzdaten für das aktuelle Symbol einlesen\n","    df_finance = spark.read.table(f\"finance_lakehouse.dbo.{symbol}_data_silver\")\n","    \n","    # Runde den Zeitstempel auf das nächste 1-Stunden-Intervall\n","    df_finance = df_finance.withColumn(\n","        \"timestamp_1h\",\n","        (col(\"timestamp\").cast(\"long\") / 3600).cast(\"long\") * 3600\n","    ).withColumn(\n","        \"timestamp_1h\",\n","        col(\"timestamp_1h\").cast(\"timestamp\")\n","    )\n","    \n","    # Aggregiere Finanzdaten auf 1-Stunden-Intervalle\n","    df_finance_agg = df_finance.groupBy(\"symbol\", \"timestamp_1h\").agg(\n","        last(\"close\").alias(\"close\"),\n","        avg(\"volume\").alias(\"volume\"),\n","        avg(\"percentage_change\").alias(\"percentage_change\")\n","    ).orderBy(\"timestamp_1h\")\n","    \n","    # Mit ersten Prognosen joinen\n","    df_combined = df_finance_agg.join(\n","        first_predictions_df,\n","        [\"symbol\", \"timestamp_1h\"],\n","        \"left\"\n","    )\n","    \n","    # Allgemeine Marktstimmung als Feature hinzufügen\n","    df_combined = df_combined.withColumn(\n","        \"overall_market_sentiment\",\n","        col(\"timestamp_1h\").cast(\"double\").cast(\"long\") * 0 + overall_market_sentiment\n","    )\n","    \n","    # Feature-Engineering (erneut, aber mit Marktstimmung)\n","    window_spec = Window.partitionBy(\"symbol\").orderBy(\"timestamp_1h\")\n","    \n","    # Lagged Features (z. B. close der letzten 5 Stunden)\n","    for i in range(1, 6):\n","        df_combined = df_combined.withColumn(\n","            f\"close_lag_{i}\",\n","            lag(col(\"close\"), i).over(window_spec)\n","        ).withColumn(\n","            f\"volume_lag_{i}\",\n","            lag(col(\"volume\"), i).over(window_spec)\n","        )\n","    \n","    # Gleitender Durchschnitt (5-Stunden-Durchschnitt von close)\n","    df_combined = df_combined.withColumn(\n","        \"close_ma_5\",\n","        avg(col(\"close\")).over(window_spec.rowsBetween(-4, 0))\n","    )\n","    \n","    # Zielvariable: close-Preis der nächsten Stunde\n","    df_combined = df_combined.withColumn(\n","        \"future_close_1h\",\n","        lead(col(\"close\"), 1).over(window_spec)\n","    )\n","    \n","    # Fehlende Werte behandeln (nur für Feature-Spalten)\n","    feature_columns = [\n","        \"close\", \"volume\", \"percentage_change\",\n","        \"close_lag_1\", \"close_lag_2\", \"close_lag_3\", \"close_lag_4\", \"close_lag_5\",\n","        \"volume_lag_1\", \"volume_lag_2\", \"volume_lag_3\", \"volume_lag_4\", \"volume_lag_5\",\n","        \"close_ma_5\",\n","        \"predicted_future_close\",\n","        \"overall_market_sentiment\"\n","    ]\n","    df_combined = df_combined.fillna(0, subset=feature_columns)\n","    \n","    # VectorAssembler für die Features\n","    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n","    df_features = assembler.transform(df_combined)\n","    \n","    # Für das Training: Zeilen entfernen, bei denen future_close_1h null ist\n","    df_features_for_training = df_features.filter(col(\"future_close_1h\").isNotNull())\n","    \n","    # Train-Test-Split (z. B. 80% Training, 20% Test)\n","    train_data, test_data = df_features_for_training.randomSplit([0.8, 0.2], seed=42)\n","    \n","    # XGBoost-Modell (in Spark als GBTRegressor implementiert)\n","    gbt = GBTRegressor(\n","        featuresCol=\"features\",\n","        labelCol=\"future_close_1h\",\n","        maxIter=50,\n","        maxDepth=5,\n","        seed=42\n","    )\n","    \n","    # Modell trainieren\n","    model = gbt.fit(train_data)\n","    \n","    # Prognosen auf Testdaten erstellen\n","    predictions = model.transform(test_data)\n","    \n","    # Evaluierung\n","    evaluator = RegressionEvaluator(labelCol=\"future_close_1h\", predictionCol=\"prediction\", metricName=\"rmse\")\n","    rmse = evaluator.evaluate(predictions)\n","    print(f\"RMSE für {symbol} (Finale Prognose): {rmse}\")\n","    \n","    # Für die Prognose: Zeilen auswählen, bei denen future_close_1h null ist\n","    latest_data = df_features.filter(col(\"future_close_1h\").isNull()).orderBy(\"timestamp_1h\", ascending=False).limit(1)\n","    \n","    # Überprüfen, ob latest_data leer ist\n","    if latest_data.count() == 0:\n","        raise ValueError(f\"Keine Daten mit future_close_1h = null für Symbol {symbol} gefunden. Überprüfe die Daten oder die Logik.\")\n","    \n","    # Inhalt von latest_data überprüfen\n","    print(f\"Inhalt von latest_data für Symbol {symbol} in Schritt 3:\")\n","    latest_data.show()\n","    \n","    latest_timestamp = latest_data.select(\"timestamp_1h\").collect()[0][\"timestamp_1h\"]\n","    \n","    # Iterativ prognostizieren für die nächsten 168 Stunden\n","    current_data = latest_data\n","    predictions_list = []\n","    \n","    for i in range(168):\n","        # Prognose für die nächste Stunde\n","        prediction = model.transform(current_data)\n","        \n","        # Prognostizierten close-Preis extrahieren\n","        predicted_close = prediction.select(\"prediction\").collect()[0][\"prediction\"]\n","        \n","        # Werte für die Feature-Spalten einmalig sammeln\n","        current_row = current_data.collect()[0]\n","        close = current_row[\"close\"]\n","        close_lag_1 = current_row[\"close_lag_1\"]\n","        close_lag_2 = current_row[\"close_lag_2\"]\n","        close_lag_3 = current_row[\"close_lag_3\"]\n","        close_lag_4 = current_row[\"close_lag_4\"]\n","        close_lag_5 = current_row[\"close_lag_5\"]\n","        volume_lag_1 = current_row[\"volume_lag_1\"]\n","        volume_lag_2 = current_row[\"volume_lag_2\"]\n","        volume_lag_3 = current_row[\"volume_lag_3\"]\n","        volume_lag_4 = current_row[\"volume_lag_4\"]\n","        volume_lag_5 = current_row[\"volume_lag_5\"]\n","        predicted_future_close = current_row[\"predicted_future_close\"]\n","        close_ma_5 = (close_lag_1 + close_lag_2 + close_lag_3 + close_lag_4 + predicted_close) / 5\n","        \n","        # Neue Zeile für die Prognose erstellen (ohne die features-Spalte im Tupel)\n","        new_timestamp = latest_timestamp + timedelta(hours=i + 1)\n","        new_row_data = [\n","            symbol,\n","            new_timestamp,\n","            predicted_close,\n","            0.0,\n","            0.0,\n","            predicted_future_close,\n","            overall_market_sentiment,\n","            close_lag_1,\n","            close_lag_2,\n","            close_lag_3,\n","            close_lag_4,\n","            close_lag_5,\n","            volume_lag_1,\n","            volume_lag_2,\n","            volume_lag_3,\n","            volume_lag_4,\n","            volume_lag_5,\n","            close_ma_5,\n","            None\n","        ]\n","        \n","        # Schema ohne die features-Spalte erstellen\n","        new_row_schema = StructType([field for field in df_features.schema if field.name != \"features\"])\n","        new_row = spark.createDataFrame([new_row_data], schema=new_row_schema)\n","        \n","        # VectorAssembler anwenden, um die features-Spalte hinzuzufügen\n","        assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n","        new_row = assembler.transform(new_row)\n","        \n","        # Prognose speichern\n","        predictions_list.append(spark.createDataFrame([(\n","            symbol,\n","            new_timestamp,\n","            close,\n","            predicted_future_close,\n","            predicted_close\n","        )], schema=[\"symbol\", \"timestamp_1h\", \"close\", \"first_predicted_future_close\", \"final_predicted_future_close\"]))\n","        \n","        # Features für die nächste Iteration aktualisieren\n","        current_data = new_row\n","    \n","    # Prognosen für das aktuelle Symbol zusammenführen\n","    symbol_predictions = predictions_list[0]\n","    for df in predictions_list[1:]:\n","        symbol_predictions = symbol_predictions.union(df)\n","    \n","    final_predictions.append(symbol_predictions)\n","\n","# Alle finalen Prognosen zusammenführen\n","final_predictions_df = final_predictions[0]\n","for df in final_predictions[1:]:\n","    final_predictions_df = final_predictions_df.union(df)\n","\n","# Ergebnisse in der Gold Layer speichern\n","final_predictions_df.write.mode(\"overwrite\").saveAsTable(\"finance_lakehouse.dbo.stock_predictions_gold\")\n","\n","# Ergebnis anzeigen\n","final_predictions_df.show(20)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"75d6c18a-c8de-48b4-b3b3-d3e66e94e1b0"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"environment":{},"lakehouse":{"known_lakehouses":[{"id":"1631f146-6ab7-4acf-b8ea-619eba023189"}],"default_lakehouse":"1631f146-6ab7-4acf-b8ea-619eba023189","default_lakehouse_name":"finance_lakehouse","default_lakehouse_workspace_id":"e351f4fd-db59-4c5e-9ef7-9f3e6ee9a6b4"}}},"nbformat":4,"nbformat_minor":5}